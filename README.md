ğŸ—ï¸ Architecture & Flow
1.	Load environment variables using python-dotenv
2.	Retrieve the GROQ_API_KEY from the environment
3.	Initialize the Groq LLM via LangChain
4.	Send a prompt to the model
5.	Stream the response in real-time
6.	Print the generated tokens progressively
________________________________________
âš™ï¸ Tech Stack
â€¢	Python 3.x
â€¢	LangChain
â€¢	Groq API
â€¢	python-dotenv
â€¢	LLaMA 3.3 70B Versatile Model
________________________________________
ğŸ“¦ Dependencies
â€¢	langchain-groq
â€¢	python-dotenv
Install dependencies:
pip install langchain-groq python-dotenv
________________________________________
ğŸ” Environment Configuration
Create a .env file in the root directory:
GROQ_API_KEY=your_api_key_here
The script verifies whether the key is loaded correctly before making API calls.
________________________________________
ğŸš€ Key Features
â€¢	âœ… Secure API key management using environment variables
â€¢	âœ… Real-time token streaming from LLM
â€¢	âœ… Minimal and clean implementation
â€¢	âœ… Warning suppression for clean console output
â€¢	âœ… Easily extensible for chatbot or AI agent systems
________________________________________
ğŸ§  Implementation Details
1ï¸âƒ£ Environment Loading
load_dotenv()
api_key = os.getenv("GROQ_API_KEY")
Loads environment variables securely and verifies key availability.
2ï¸âƒ£ Model Initialization
llm = ChatGroq(model='llama-3.3-70b-versatile')
Initializes the Groq-hosted LLaMA 3.3 70B versatile model.
3ï¸âƒ£ Streaming Response
for reply in llm.stream('Who Is God ?'):
    print(reply.content, end='', flush=True)
Uses LangChain's streaming interface to receive and print output incrementally.
This reduces perceived latency and improves user experience for interactive applications.
________________________________________
ğŸ”„ Execution Flow
Run the script:
python app.py
Expected Output:
Key loaded: True
<streamed LLM response appears progressively>
________________________________________
ğŸ“ˆ Use Cases
â€¢	AI Chatbots
â€¢	Real-time LLM-based CLI tools
â€¢	Research assistants
â€¢	Agentic AI systems
â€¢	Low-latency AI applications

